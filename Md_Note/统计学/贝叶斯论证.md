# 贝叶斯论证

标签：读书笔记 概率论

书目： [Korb, K and Nicholson, A. (2011). *Bayesian Artificial Intelligence*. (2nd Edition) CRC Press](https://www.amazon.com/Bayesian-Artificial-Intelligence-Computer-Analysis/dp/1439815917)

## Chapter 1 Bayesian Reasoning

## 1.1 An Brief Introduction

> "...widespread recognition that practical AI systems shall have to **cope with uncertainty** ... deal with incomplete evidence leading to beliefs that fall **short of knowledge** ... general-purpose AI will need to be able to reason probabilistically, what we call here **Bayesian reasoning**."

* **不确定性(uncertainty)**

  3种形式的不确定性：
  * Ignorance: 由认知的局限性导致的。
  * Physical randomness or indeterminism: 由自然规律中的随机性或不确定性导致的。
  * Vagueness：由公理或定理导致的界限模糊。

Bayesianism贝叶斯思想体系认为，表达某一猜测的最好方法是用概率公式（the probability calculus）进行表达。利用概率论来解释因果关系，或使智能Agent能够对其所属的物理世界进行论证分析。

在AI发展的早期，大多数研究是基于逻辑和知识库来对问题进行论证，但在实际解决问题过程中，**利用逻辑论证(logic reasoning)并不足以解决以不确定性为特性的问题**，特别是针对NP问题，因此学者和科学家将目光转向**朴素贝叶斯(Naive Bayes)**。

**确定因素(Certainty factors, CFs)**可认为是某一资料或信息(evidence) $e$ 使得某一猜测(hypothesis) $h$ 为真的概率产生变动。这种变动可以为增加该猜测为真的可能性（increase belief）记为$MB(h,e),\in[0,1]$，或 增加该猜测为假的可能性（increase disbelief）记为$MD(h,e),\in[0,1]$，则可用以下等式来表达确定因素对某猜测的影响：$CF(h,e)=MB(h,e)-MD(h,e), \in[-1,1]$。但是这种形式的表达并不能直观准确地表达到底 $e$ 对猜测 $h$ 的影响，且需要严格地假设各种信息 $e$ 是彼此独立的。而贝叶斯网络恰好能满足这个假设，且对概率有更为自然的描述。

---

* 概率公式(Probability Calculus):

  概率论中的**柯尔莫哥洛夫3大公理(Kolmogorov's axiom)**：

  * **第一公理**—非负性：对于有限事件空间$F$ ，$\forall X\subseteq F, P(X)\ge 0, X\in F$
  * **第二公理**—规范性：对于事件全集$\Omega$，$P(\Omega )=1$
  * **第三公理**—可加性：对于任意有限互斥子集{$E_{i}$}，$p(\bigcup_{i=1}^{\infty}{E_{i}})=\sum_{i=1}^{\infty}{P(E_{i})}$
    * 或另一种表达：$\forall X,Y \subseteq \Omega$， 若$X \cap Y = \emptyset$，则$P(X\cup Y)=P(X)+P(Y)$

  **推论**：

  * 空集：$P(\emptyset)=0$
  * 单调性：若$ A\subseteq B $ 则 $P(A)\le P(B)$
  * 有界性：$0\le P(E)\le1$
  * 容斥定理（二元）：$P(A\cup B)=P(A)+P(B)-P(A\cap B)$

---

* **条件概率(Conditional Probability)与独立**：

  $$P(A|B)=\cfrac{P(A\cap B)}{P(B)}$$

  ![交集](http://jedi.org/blog/archives/a-and-b.png)

  若已知已经发生或将会发生的事件B（$P(B)\ne 0$），上式表达的则为在已知有关事件B的信息 $e$ 的背景下，A会发生的概率。即之前发生的事件B影响之后发生的事件A的概率。

  >  **理解**：
  >
  > 可想象为当得知信息 $e$ 后，此时时间全集 $\Omega$ 的范围缩小至与B相同的大小。而在事件B的前提下，A将会发生的概率等于A，B的交集在事件B集合中所占的比例。

  * **独立**：若事件A，B概率层面上彼此独立，可记为 $A \bot B$，满足交换律$A \bot B \equiv B \bot A $，且有$A \bot B \equiv P(A|B)=P(A)$。

  * **条件独立**：若事件A，B概率层面上彼此独立，现有事件Z，且有$A\cap Z \ne \empty$，则$A \bot B| Z\equiv P(A|B,Z)=P(A|Z)$。

    > **理解**：因$A \bot B$，事件B并没有提供许多关于A的信息(uninformative)， 而$A\cap Z \ne  \empty$表明Z所包含的所有子事件中能提供更多关于A的信息，因此可认为B在改等式中不含信息，因此可以帮这个B省去。

  * **条件概率公式**：

    * **统一性**：

      若有事件集{$A_{i}$}，$\forall A_{i}\cap A_{j}=\empty $且，$\bigcup _{i=1}^{n}{A_{i}}=\Omega$（即事件A充满了全集），则：

      $P(\Omega)=\sum_{i=1}^{n}{P(A_{i})}$。

      且因$\bigcup _{i=1}^{n}{A_{i}}=\Omega$，若$\forall i, A_{i} \ne \empty$，则：

      $P(B)=\sum_{i=1}^{n}{P(B|A_{i})}$。

    * **链式法则**：

      若有事件A，B，C，相互不独立，则：

      $P(C|A)=P(C|B)P(B|A)+P(C|\neg B)P(\neg B|A)$

      >**理解**：上式表示在事件A发生的前提下，C发生的概率。其中插入了中间事件B，且$B\ne \empty$。因中间事件B的发生与否（$B$和$\neg B$）都不影响事件A所包含的信息，因此B对C的条件概率没有影响（可以理解为事件B只是一个中间产物），可以从上式中发现，事件B的正反事件都被包含了。

---

* **贝叶斯定理（Bayes's theorem）**

  由条件概率得：

  $$P(h|e)=\cfrac{P(e|h)P(h)}{P(e)}\Leftrightarrow P(h,e)=P(e|h)P(h)=P(h|e)P(e)$$

  注意上式左侧等式中，分子$P(e|h)p(h)$表示当前合事件$P(h,e)$发生的概率，分母$P(e)$则表示根据以往经验得知的事件 $e$ 的发生概率，即为已知资料。上式通过条件概率公式对当前发生的合事件$P(e,h)$进行**标准化（normalise）**。更准确的描述则是，根据当前合事件$P(e,h)$发生的概率，以及过往经验总结的事件 $e$ 发生的概率，对事件 $h$ 给定资料 $e$ 时的条件概率**进行调整**，这个过程叫做**条件化（Conditionalization）**。

  条件化是**根据已知资料和某当前事件的概率，对事件发生的条件概率进行调整的过程**。上式中$P(e)$ 称为**先验概率（prior probability）**，信息 $e$ 表示跟据过往资料得知的某事件发生概率（观测数据前）。$P(h|e)$称为**后验概率（posterior probability）**，为根据当前发生的合事件$P(h,e)$以及信息 $e$ ，对事件 $h$ 的条件概率调整后的概率。

  使用条件化必须符合两个假设：

  * 事件之间的交集为非空。
  * 运算中只考虑信息 $e$ ，其他信息不参与。

  > **例**：根据以往经验，某机器调整良好时，产品合格率为98%；若出现故障（没调整好）则产品合格率为55%。若每天机器调整良好的概率为95%，已知某天第一件产品为合格时，该机器的状态为调整良好的概率是多少？
  >
  > **解**：“某机器调整良好时，产品合格率为98%”是先验概率，或称之为根据过往经验总结的经验概率；"每天机器调整良好的概率为95%"则为过往信息。现设事件A为“产品合格”，事件B为“调整良好”。
  >
  > 根据已知则有：$P(A|B)=0.98$，$P(A|\bar{B})=0.55$，$P(B)=0.95$，$P(\bar{B})=0.05$。
  >
  > “产品合格”发生概率：$P(A)=P(A|B)P(B)+P(A|\bar{B})P(B)=0.98\times0.95+0.55\times 0.05=0.9585$
  >
  > **注意**：此时求出的事件A的发生概率为**根据以往数据求出的事件A发生的概率**。
  >
  > 现“第一件产品为合格” ，表示该事件已发生，**我们以该事件作为新的信息**。现根据条件概率来反推今天机器调整良好的概率：
  >
  > $P(B|A)=\cfrac{P(A|B)P(B)}{P(A)}=\cfrac{0.98\times0.95}{0.9585}=0.97$
  >
  > 计算结果显示，根据新的信息，“机器调整良好”这一事件发生的概率为97%。

---

* 投注赔率（发生比）

  $某事件的发生比=\cfrac{某事件发生的概率}{某事件不发生的概率}$

  $O(A)=\cfrac{P(A)}{1-P(A)}\Leftrightarrow P(A)=\cfrac{O(A)}{1+O(A)}$

  * 发生比中贝叶斯定理的应用

    由贝叶斯公式$P(h|e)=\cfrac{P(e|h)P(h)}{P(e)}$ 和全概率公式 $P(e)=P(e|h)P(h)+P(e|\neg h)P(\neg h)$ 得：

    $1-P(h|e)=\cfrac{P(e)-P(e|h)P(h)}{P(e)}=\cfrac{P(e|\neg h)P(\neg h)}{P(e)}$

    $O(h|e)=\cfrac{P(h|e)}{1-P(h|e)}=\cfrac{P(e|h)P(h)}{P(e|\neg h)P(\neg h)}=\cfrac{P(e|h)}{P(e|\neg h)}\times \cfrac{P(h)}{1-P(h)}=\cfrac{P(e|h)}{P(e|\neg h)} \cdot O(h) $

    $\Rightarrow O(h|e)=\cfrac{P(e|h)}{P(e|\neg h)} \cdot O(h)$

    $\cfrac{P(e|h)}{P(e|\neg h)}$为事件 $(e|h)$ 与事件 $(e|\neg h)$ 的概率比。可以发现，仅当$\cfrac{P(e|h)}{P(e|\neg h)}\ge1$时，$O(h|e)\ge O(h)$。

    

    

  

  









